Systematically scan the codebase for pending, deferred, or incomplete implementation items using comprehensive search patterns. Update existing items in PENDING_ITEMS.md and add newly discovered items.

## Search Strategy

Execute parallel searches using these patterns with **FALSE POSITIVE PREVENTION**:

### Implementation Gap Patterns

**NotImplementedError patterns:**

```bash
rg "NotImplementedError" --type py -n -C 2
```

**Empty method stubs:**

```bash
rg "pass\s*$" --type py -n -C 2
rg "pass\s*#" --type py -n -C 1  # Pass with explanatory comments
```

**TODO markers:**

```bash
rg -i "(TODO|FIXME|HACK|XXX|BUG|NOTE)" --type py -n -C 2
```

**Placeholder/temporary patterns:**

```bash
rg -i "(placeholder|temporary|temp|stub|mock)" --type py -n -C 2 | grep -v "test_"  # Exclude test files
```

**Deferred implementation:**

```bash
rg -i "(later|eventually|defer|postpone|pending|phase.*[0-9])" --type py -n -C 2
```

### Validation Protocol (MANDATORY)

**STEP 1: Initial Pattern Detection**
Execute comprehensive search patterns and create a candidate list of potential gaps.

**STEP 2: Parallel Sub-Agent Validation (CRITICAL)**
For each high-impact finding, use multiple parallel Task calls to evaluate implementation status:

```
Task calls should analyze:
- Does the method/class have working implementation?
- Are there related components that provide this functionality?
- Is there supporting infrastructure that enables this feature?
- Is this missing core logic or just missing wiring/integration?

Return assessment: IMPLEMENTED/PARTIAL/WIRING_ONLY/NOT_IMPLEMENTED with evidence
```

**STEP 3: Implementation vs Wiring Classification**

**ðŸ”´ ARCHITECTURAL GAPS (XL-L effort)**:

- Complete missing systems (validation interfaces, state management)
- Missing core business logic or algorithms
- Absent foundational infrastructure
- No concrete implementations exist anywhere

**ðŸŸ¡ WIRING GAPS (S-XS effort)**:

- Implementation exists but not injected/connected
- Infrastructure present but domain methods are stubs
- Repository patterns missing but models/interfaces exist
- Configuration or dependency injection gaps

**ðŸŸ¢ ENHANCEMENT GAPS (XS-S effort)**:

- Basic functionality works but missing advanced features
- Library integration needed (dependencies exist)
- Performance or UX improvements
- Feature completeness rather than core functionality

**STEP 4: Evidence-Based Verification**

**Before marking as NOT_IMPLEMENTED:**

1. **Verify Implementation Alternatives:**

```bash
rg "class.*${PATTERN}" --type py   # Search for related implementations
rg "def ${METHOD_NAME}" --type py  # Check if method exists elsewhere
```

2. **Check Intentional Design:**

```bash
rg "(NoOp|Mock|Stub).*class" --type py      # Phase 1 intentional patterns
rg "# (Phase|MVP|intentional)" --type py -C 1   # Design intent comments
```

3. **Validate Current State:**

```bash
sed -n '${LINE_NUMBER}p' ${FILE_PATH}       # Verify line numbers accurate
git log --oneline -5 -- ${FILE_PATH}       # Check recent changes
```

4. **Exclude False Positives:**
   - **Test Files**: Skip `test_*` files and `tests/` directory
   - **Documentation**: Skip inline code examples
   - **Phase Markers**: Future phase items may not be current blockers
   - **Alternative Implementations**: Different class/method names

## Parallel Task Validation Strategy

**STEP 5: Batch Task Evaluation**
Execute multiple Task calls in a single message to evaluate different aspects:

### Example Multi-Task Pattern:

```markdown
I'll evaluate the remaining items in parallel using sub-agents:

<Task subagent_type="general-purpose" description="Evaluate HCL parsing">
Analyze HCL syntax parsing in config_domain.py:55. Check if ProcessHCLParser._parse_hcl_content() exists and works.
Focus on: core_engine/config/parser.py, domain integration
Return: IMPLEMENTED/WIRING_ONLY/NOT_IMPLEMENTED with evidence
</Task>

<Task subagent_type="general-purpose" description="Evaluate FormConfig system">  
Analyze FormConfig retrieval at line 200. Check if FormConfigGenerator exists and if database models support persistence.
Focus on: core_engine/forms/generator.py, database models
Return: Status assessment with wiring vs implementation gaps
</Task>

<Task subagent_type="general-purpose" description="Evaluate data source execution">
Analyze data source execution. Check if DataSourceExecutor exists and works.
Focus on: core_engine/data_sources/executor.py, domain integration
Return: Implementation completeness and wiring requirements  
</Task>
```

### Task Analysis Framework:

**Each sub-agent should assess:**

1. **Infrastructure Existence**: Does supporting infrastructure exist?
2. **Implementation Completeness**: Are core methods implemented?
3. **Integration Status**: Are components wired together?
4. **Test Coverage**: Do tests demonstrate working functionality?
5. **Gap Classification**: Wiring vs architectural vs enhancement gap?

## Updated Classification System

**RESOLVED Categories:**

- **âœ… IMPLEMENTED**: Fully working with tests
- **âœ… WORKING AS INTENDED**: NoOp by design for current phase

**Gap Categories:**

- **ðŸ”´ ARCHITECTURAL GAP**: Missing core systems/algorithms
- **ðŸŸ¡ WIRING GAP**: Implementation exists, needs connection
- **ðŸŸ¢ ENHANCEMENT GAP**: Basic works, advanced features missing
- **ðŸ”µ INTEGRATION GAP**: Components exist, missing library/dependency
- **âšª PHASE DEFERRED**: Intentionally postponed to future phases

## Priority Guidelines (Updated)

**CRITICAL**:

- Raises exceptions in main execution path
- Blocks core walking skeleton functionality
- Security vulnerabilities in production paths

**HIGH**:

- Degrades functionality significantly (mock data, incomplete features)
- Missing important user-facing capabilities
- Integration gaps that prevent feature completion

**MEDIUM**:

- Enhancement opportunities (performance, UX)
- Advanced features beyond MVP scope
- Quality of life improvements

**LOW**:

- Technical debt (code quality, optimization)
- Future-proofing improvements
- Documentation and tooling enhancements

**STEP 6: Results Integration**

Update PENDING_ITEMS.md entries with refined classifications:

### Status Field Options:

- **Implemented**: Fully working (mark as âœ… RESOLVED)
- **Working as Intended**: Phase 1 NoOp by design (mark as âœ… RESOLVED)
- **Wiring Gap**: Implementation exists, needs connection
- **Integration Gap**: Components exist, needs library/dependency
- **Enhancement Gap**: Basic works, advanced features missing
- **Not Implemented**: True architectural gap
- **Phase Deferred**: Intentionally postponed

### Effort Recalibration:

- **Architectural gaps**: Keep original XL-L estimates
- **Wiring gaps**: Reduce to S-XS (simple dependency injection)
- **Integration gaps**: Reduce to XS-S (library addition + simple wrapper)
- **Enhancement gaps**: Keep S-M estimates

## Validation Success Metrics

A successful pending items analysis should:

- **Distinguish implementation vs wiring** for 80%+ of findings
- **Resolve 20-30%** of initially identified gaps through discovery of existing implementations
- **Accurately classify effort** by separating complex development from simple integration
- **Provide evidence** for each assessment with specific file/line references
- **Update totals** to reflect realistic development effort rather than pessimistic estimates

## Example Output Format

```markdown
#### **ðŸŸ¡ WIRING GAP - Data Source Execution**

- **File**: `core_engine/domains/configuration_domain.py:213`
- **Context**: DataSourceExecutor exists but not wired to ConfigurationDomain
- **Status**: Wiring Gap
- **Effort**: S
- **Last Updated**: 2025-08-28
- **Description**: Implementation exists, needs dependency injection in **init**
```

@PENDING_ITEMS.md
